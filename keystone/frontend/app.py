import streamlit as st
import sys
import os
import tempfile
import json
import logging
from datetime import datetime
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px

# Add parent directory to path to import backend modules
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

try:
    from backend.document_processor import DocumentProcessor
    from backend.vector_store import VectorStore
    from backend.claim_extractor import ClaimExtractor
    from backend.fact_verifier import FactVerifier
    from backend.correction_engine import CorrectionEngine
except ImportError as e:
    st.error(f"Failed to import backend modules: {e}. Please ensure you are running this from the 'frontend' directory or set PYTHONPATH correctly.")
    st.stop()

# Page config
st.set_page_config(
    page_title="Keystone - AI Fact Checker",
    page_icon="üîë",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for modern styling
st.markdown("""
<style>
    /* Main theme colors */
    :root {
        --primary-color: #06B6D4;
        --success-color: #10B981;
        --warning-color: #F59E0B;
        --danger-color: #EF4444;
    }
    
    /* Header styling */
    .main-header {
        font-size: 3rem;
        font-weight: 800;
        background: linear-gradient(135deg, #06B6D4 0%, #3B82F6 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 1rem;
    }
    
    /* Status badges */
    .status-supported {
        background-color: #10B981;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 600;
        display: inline-block;
        font-size: 0.9em;
    }
    
    .status-contradicted {
        background-color: #EF4444;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 600;
        display: inline-block;
        font-size: 0.9em;
    }
    
    .status-unverifiable {
        background-color: #F59E0B;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 600;
        display: inline-block;
        font-size: 0.9em;
    }
    
    .status-partially {
        background-color: #8B5CF6;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 600;
        display: inline-block;
        font-size: 0.9em;
    }
    
    /* Claim highlighting */
    .claim-supported {
        background-color: rgba(16, 185, 129, 0.1);
        border-left: 4px solid #10B981;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 8px;
    }
    
    .claim-contradicted {
        background-color: rgba(239, 68, 68, 0.1);
        border-left: 4px solid #EF4444;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 8px;
    }
    
    .claim-unverifiable {
        background-color: rgba(245, 158, 11, 0.1);
        border-left: 4px solid #F59E0B;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 8px;
    }
    
    /* Evidence box */
    .evidence-box {
        background: #F8FAFC;
        border: 1px solid #E2E8F0;
        border-radius: 6px;
        padding: 0.75rem;
        margin: 0.5rem 0;
        font-size: 0.9em;
        color: #334155;
    }
    
    /* Progress bar customization */
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #06B6D4, #3B82F6);
    }
</style>
""", unsafe_allow_html=True)

# --------------------------------------------------------------------------
# SIDEBAR
# --------------------------------------------------------------------------
with st.sidebar:
    st.markdown("# üîë Keystone")
    st.markdown("**AI-Powered Fact Checking**")
    st.markdown("---")
    
    # API Key input
    api_key_input = st.text_input(
        "AI Provider API Key",
        type="password",
        value=st.session_state.get('api_key', ''),
        help="Enter your Anthropic or OpenAI API key for claim extraction"
    )
    if api_key_input:
        st.session_state.api_key = api_key_input
        # Simple heuristic to guess provider, or add dropdown. Default to OpenAI compatible or what backend expects.
        # User prompt asked for "Anthropic API Key".
    
    st.markdown("### üìÑ Upload Source Document")
    uploaded_file = st.file_uploader(
        "Upload trusted source (PDF/TXT)",
        type=['pdf', 'txt'],
        help="This is your ground truth document"
    )
    
    st.markdown("### üìù LLM Generated Text")
    llm_output = st.text_area(
        "Paste LLM output to verify",
        height=200,
        placeholder="Paste the text generated by an LLM that you want to fact-check..."
    )
    
    st.markdown("---")
    
    # Advanced options (collapsible)
    with st.expander("‚öôÔ∏è Advanced Settings"):
        top_k_retrieval = st.slider("Evidence retrieval (top-k)", 1, 10, 5)
        # confidence_threshold = st.slider("Confidence threshold", 0.0, 1.0, 0.65, 0.05) 
        # (FactVerifier has hardcoded thresholds, passing this would require updating FactVerifier init or method signature)
        show_detailed_evidence = st.checkbox("Show detailed evidence", value=True)
    
    # Verify button
    verify_button = st.button("üöÄ Verify Facts", type="primary", use_container_width=True)
    
    st.markdown("---")
    st.markdown("### üìä Quick Stats")
    if 'results' in st.session_state and st.session_state.results:
        total = len(st.session_state.results)
        supported_count = sum(1 for r in st.session_state.results if r['status'] == 'supported')
        st.metric("Total Claims", total)
        st.metric("Supported", supported_count, delta=f"{supported_count/total*100:.0f}%" if total > 0 else "0%")

# --------------------------------------------------------------------------
# MAIN CONTENT
# --------------------------------------------------------------------------

# Main header
st.markdown('<h1 class="main-header">üîë Keystone</h1>', unsafe_allow_html=True)
st.markdown("### Automated Fact-Checking & Attribution for LLMs")
st.markdown("---")

# Logic
if not uploaded_file:
    st.info("üëà Upload a source document in the sidebar to begin")
    
    # Show demo/features
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("#### üéØ Claim Detection")
        st.write("Automatically extracts atomic factual claims from LLM outputs using AI.")
    with col2:
        st.markdown("#### üîç RAG Verification")
        st.write("Verifies each claim against your trusted knowledge base using NLI.")
    with col3:
        st.markdown("#### üìä Visual Reports")
        st.write("Interactive dashboard with color-coded results and citations.")

elif not llm_output:
    st.info("üëà Paste LLM-generated text in the sidebar to verify")

elif verify_button:
    # Check API key if needed by ClaimExtractor (Rule-based fallback exists, but warning is better)
    if not st.session_state.get('api_key'):
        st.warning("‚ö†Ô∏è No API Key provided. Claim extraction will rely on rule-based fallback (less accurate).")

    # Initialize results container
    st.session_state.results = []
    
    # 1. PROCESSING
    with st.spinner("üìÑ Processing source document..."):
        try:
            processor = DocumentProcessor()
            
            # Extract text based on file type
            if uploaded_file.name.endswith('.pdf'):
                # Save temporarily
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                    tmp.write(uploaded_file.read())
                    tmp_path = tmp.name
                documents = processor.process_file(tmp_path) 
                # Note: process_file returns chunks directly in previous implementations, 
                # but let's check. Wait, processor.process_file -> returns chunks list[dict].
                # User prompt code snippet used extract_text_from_pdf manually.
                # Let's use the high level `process_file` which handles chunking internally if possible 
                # OR follow prompt strictly. 
                # Prompt said: documents = processor.extract_text_from_pdf(tmp_path) then chunk_documents.
                # DocumentProcessor.process_file calls extract then chunk.
                # We'll use result of process_file as 'chunks'.
                
                # Re-reading prompt logic:
                # documents = processor.extract_text_from_pdf(tmp_path)
                # chunks = processor.chunk_documents(documents)
                # Actually process_file does both. Let's trust process_file.
                
                # Use high level API for robustness
                chunks = processor.process_file(tmp_path)
                os.remove(tmp_path) # Cleanup
                
            else:
                # TXT
                content = uploaded_file.read().decode('utf-8')
                # Create a structure mimicking what extract_* returns if we want to use chunk_documents manually,
                # or just save to temp file and use process_file
                with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp:
                    tmp.write(content.encode('utf-8'))
                    tmp_path = tmp.name
                chunks = processor.process_file(tmp_path)
                os.remove(tmp_path)

            if not chunks:
                st.error("No text could be extracted or chunked from the document.")
                st.stop()
                
            st.success(f"‚úÖ Processed document: {len(chunks)} chunks created.")
            
        except Exception as e:
            st.error(f"Error processing document: {e}")
            st.stop()
    
    # 2. VECTOR STORE
    with st.spinner("üóÑÔ∏è Building knowledge base..."):
        try:
            # Create a unique collection name to avoid conflicts or reuse appropriately
            # For this session app, maybe ephemeral? But VectorStore is persistent.
            # We'll use a session-specific name or random
            collection_name = f"keystone_session_{int(datetime.now().timestamp())}"
            vector_store = VectorStore(collection_name=collection_name)
            # Clear if exists (unlikely given timestamp)
            # vector_store.delete_collection() # Optional safety
            
            vector_store.add_documents(chunks)
            stats = vector_store.get_collection_stats()
            st.success(f"‚úÖ Indexed {stats.get('total_chunks', 0)} chunks into '{collection_name}'")
        except Exception as e:
            st.error(f"Error building vector store: {e}")
            st.stop()
    
    # 3. CLAIM EXTRACTION
    with st.spinner("üéØ Extracting atomic claims..."):
        try:
            extractor = ClaimExtractor(api_key=st.session_state.get('api_key'))
            claims = extractor.extract_atomic_claims(llm_output)
            st.success(f"‚úÖ Extracted {len(claims)} claims")
        except Exception as e:
            st.error(f"Error extracting claims: {e}")
            st.success("Proceeding with whole text as single claim fallback...")
            claims = [{'claim': llm_output, 'claim_id': 'fallback', 'type': 'text'}]

    # 4. VERIFICATION
    with st.spinner("üîç Verifying facts..."):
        verifier = FactVerifier(vector_store)
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        results = []
        for idx, claim_obj in enumerate(claims):
            status_text.text(f"Verifying claim {idx+1}/{len(claims)}...")
            
            claim_text = claim_obj.get('claim', '')
            if not claim_text: continue
            
            result = verifier.verify_claim(claim_text, top_k=top_k_retrieval)
            
            # Enrich result with metadata
            result['claim_id'] = claim_obj.get('claim_id', f'claim_{idx}')
            result['claim_type'] = claim_obj.get('claim_type', 'factual')
            
            results.append(result)
            progress_bar.progress((idx + 1) / len(claims))
            
        # Post-processing for Corrections
        # We do this after main verification to keep progress bar moving for initial results
        correction_engine = CorrectionEngine(api_key=st.session_state.get('api_key'))
        
        for res in results:
            if res['status'] == 'contradicted' and not res.get('suggestion'):
                status_text.text(f"Generating correction for: {res['claim'][:30]}...")
                # Get top evidence
                top_evidence = res['evidence'][0]['text'] if res['evidence'] else ""
                if top_evidence:
                    correction = correction_engine.generate_correction(res['claim'], top_evidence)
                    if correction:
                        res['suggestion'] = correction
        
        st.session_state.results = results
        progress_bar.empty()
        status_text.empty()
        
        # Cleanup vector store logic if desired? Keep for now to allow re-verify?
        # Ideally we keep it for the session.

    # ----------------------------------------------------------------------
    # RESULTS DISPLAY
    # ----------------------------------------------------------------------
    
    st.markdown("---")
    st.header("üìä Verification Results")
    
    if not results:
        st.warning("No claims verified.")
    else:
        # Metrics
        total_claims = len(results)
        supported = sum(1 for r in results if r['status'] == 'supported')
        contradicted = sum(1 for r in results if r['status'] == 'contradicted')
        unverifiable = sum(1 for r in results if r['status'] == 'unverifiable')
        partial = sum(1 for r in results if r['status'] == 'partially_supported')
        
        trust_score = (supported / total_claims * 100) if total_claims > 0 else 0
        
        col1, col2, col3, col4, col5 = st.columns(5)
        col1.metric("Total Claims", total_claims)
        col2.metric("‚úÖ Supported", supported)
        col3.metric("‚ùå Contradicted", contradicted)
        col4.metric("‚ö†Ô∏è Unverifiable", unverifiable)
        col5.metric("üéØ Trust Score", f"{trust_score:.0f}%")
        
        # Chart
        st.markdown("---")
        st.subheader("üìà Verification Breakdown")
        
        chart_data = {
            'Status': ['Supported', 'Contradicted', 'Unverifiable', 'Partial'],
            'Count': [supported, contradicted, unverifiable, partial]
        }
        fig = px.pie(
            names=chart_data['Status'], 
            values=chart_data['Count'],
            color=chart_data['Status'],
            color_discrete_map={
                'Supported': '#10B981',
                'Contradicted': '#EF4444',
                'Unverifiable': '#F59E0B',
                'Partial': '#8B5CF6'
            },
            hole=0.4,
            title="Claim Verification Distribution"
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Detailed Analysis
        st.markdown("---")
        st.subheader("üîç Detailed Claim Analysis")
        
        filter_status = st.multiselect(
            "Filter by status",
            ['supported', 'contradicted', 'unverifiable', 'partially_supported'],
            default=['supported', 'contradicted', 'unverifiable', 'partially_supported']
        )
        
        for idx, result in enumerate(results, 1):
            if result['status'] not in filter_status:
                continue
            
            # Styles
            if result['status'] == 'supported':
                status_class = 'status-supported'
                claim_class = 'claim-supported'
                emoji = '‚úÖ'
            elif result['status'] == 'contradicted':
                status_class = 'status-contradicted'
                claim_class = 'claim-contradicted'
                emoji = '‚ùå'
            elif result['status'] == 'partially_supported':
                status_class = 'status-partially'
                claim_class = 'claim-supported'
                emoji = 'üî∂'
            else:
                status_class = 'status-unverifiable'
                claim_class = 'claim-unverifiable'
                emoji = '‚ö†Ô∏è'
            
            with st.expander(f"{emoji} Claim {idx}: {result['claim'][:80]}...", expanded=(idx <= 3)):
                st.markdown(f'<div class="{claim_class}">', unsafe_allow_html=True)
                st.markdown(f"**Claim:** {result['claim']}")
                st.markdown('</div>', unsafe_allow_html=True)
                
                col_a, col_b = st.columns([1, 2])
                with col_a:
                    st.markdown(f'<div class="{status_class}">{result["status"].upper()}</div>', unsafe_allow_html=True)
                    st.markdown(f"**Confidence:** {result['confidence']:.0%}")
                    st.markdown(f"**Type:** {result.get('claim_type', 'factual')}")
                
                with col_b:
                    st.markdown("**Reasoning:**")
                    st.info(result['reasoning'])
                
                # Evidence
                if show_detailed_evidence and result.get('evidence'):
                    st.markdown("**üìö Evidence:**")
                    for ev_idx, evidence in enumerate(result['evidence'][:3], 1):
                        with st.container():
                            # Clean up text for display
                            clean_text = evidence.get('text', '').replace('\n', ' ')
                            st.markdown(f'<div class="evidence-box"><b>Source:</b> {evidence.get("source", "unknown")} (Page {evidence.get("page", 0)})<br>"{clean_text}"</div>', unsafe_allow_html=True)
                
                 # Suggestion
                if result['status'] == 'contradicted':
                    st.markdown("**üí° Suggested Correction:**")
                    if result.get('suggestion'):
                         st.success(result['suggestion'])
                    else:
                        # On-demand correction if not already present
                        # Note: This might be slow if done sequentially. 
                        # ideally we do it in the verification loop or here with a spinner.
                        # Since we didn't add it to verifier loop to avoid circular depend, we do it here?
                        # Or better: Add it to the main processing loop above.
                        # Let's add it to the main loop to be clean.
                        pass

        # Split Screen View
        st.markdown("---")
        st.subheader("üìë Visual Document View")
        
        left_col, right_col = st.columns(2)
        
        with left_col:
            st.markdown("**üìù Annotated Text**")
            # Highlight annotations
            annotated_html = llm_output
            
            # Sort by length desc to avoid replacing substrings incorrectly
            sorted_results = sorted(results, key=lambda x: len(x['claim']), reverse=True)
            
            for res in sorted_results:
                claim_txt = res['claim']
                # Determine color
                if res['status'] == 'supported': color = "#d1fae5" # light green
                elif res['status'] == 'contradicted': color = "#fee2e2" # light red
                elif res['status'] == 'partially_supported': color = "#ede9fe" # light purple
                else: color = "#fef3c7" # light yellow
                
                # Escape? Simple implementation assumes exact match text
                if claim_txt in annotated_html:
                     annotated_html = annotated_html.replace(
                        claim_txt,
                        f'<mark style="background-color: {color}; border-radius: 4px; padding: 2px;">{claim_txt}</mark>'
                     )
            
            st.markdown(
                f'<div style="padding:1rem; border:1px solid #ddd; border-radius:8px; height:400px; overflow-y:auto; line-height:1.6;">{annotated_html}</div>', 
                unsafe_allow_html=True
            )
        
        with right_col:
            st.markdown("**üìÑ Source Document Context**")
            # Show the first chunk text or all text if small
            # Just showing raw text for now as requested
            full_text = "\n\n".join([c['text'] for c in chunks])
            st.text_area("Full Source Text", value=full_text, height=400, disabled=True)

        # Export
        st.markdown("---")
        st.subheader("üì• Export Report")
        
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'source_document': uploaded_file.name,
            'trust_score': trust_score,
            'results': results
        }
        
        c1, c2 = st.columns(2)
        with c1:
            st.download_button(
                label="Download JSON",
                data=json.dumps(export_data, indent=2),
                file_name=f"keystone_report.json",
                mime="application/json"
            )
        with c2:
            # Flatten for CSV
            csv_rows = []
            for r in results:
                csv_rows.append({
                    'Claim': r['claim'],
                    'Status': r['status'],
                    'Confidence': r['confidence'],
                    'Reasoning': r['reasoning']
                })
            df_export = pd.DataFrame(csv_rows)
            st.download_button(
                label="Download CSV",
                data=df_export.to_csv(index=False),
                file_name=f"keystone_report.csv",
                mime="text/csv"
            )

elif 'results' in st.session_state and st.session_state.results:
    # State persistence for rerun (partial support mainly for metrics display if button not clicked again)
    # Ideally should restructure to not depend on button click for render logic, but sticking to prompt flow.
    pass
else:
    # Initial state hint
    pass
